# Apache Spark

[Apache Spark](https://spark.apache.org/) √© uma ferramenta para processar dados em larga escala.

Voc√™ pode estar se perguntando o que √© "larga escala" ou quando voc√™ precisa usar o Spark. A resposta √© simples: se a quantidade de dados que voc√™ quer trabalhar n√£o cabe na mem√≥ria de apenas um computador, voc√™ precisa de uma ferramenta que distribua esse processamento em v√°rias m√°quinas e consiga orquestrar esse processo, ent√£o voc√™ precisa do Spark. 

O Spark considera que um conjuto de dados √© um *Dataset*. Esses dados podem ser de qualquer tipo, inclusive classes definidas pelos usu√°rios.

Para trabalhar com os *Datasets* o Spark criou uma abstra√ß√£o chamada RDD (Reslient Distrubuted Datasets), desta forma toda a complexidade da computa√ß√£o distribu√≠da fica escondida e voc√™ executa comandos como se todos os dados estivessem juntos.

Considera√ß√µes importantes sobre RDDs:
- s√£o distribu√≠dos. Desta forma √© poss√≠vel rodar em parti√ß√µes diferentes dentro do cluster.
- s√£o imut√°veis. Assim s√£o evitados os problemas de programa√ß√£o concorrente.
- s√£o resilientes. Possibilitando serem recriados a qualquer momento, sendo tolerante a falhas.

Portanto, utilizando o Spark todo o trabalho √© definido criando um RDD, transformando um RDD e chamando opera√ß√µes e computando resultados. Abaixo vou explicar algumas fun√ß√µes b√°sicas, mas para se aprofundar mais essa [documenta√ß√£o](https://spark.apache.org/docs/latest/rdd-programming-guide.html) pode te ajudar.

### Transforma√ß√µes
S√£o fun√ß√µes aplicadas em um RDD que resulta em um novo RDD.
Exemplos:
- **Filter**: retorna um RDD formado pelos elementos que passam na fun√ß√£o de filtragem.
- **Map**: passa cada elemento do RDD de entrada atrav√©s de uma fun√ß√£o, o resultado dessa fun√ß√£o √© adicionado no RDD de sa√≠da.
- **Flatmap**: funciona da mesma forma que o *Map*, por√©m cada elemento do RDD de entrada pode gerar mais de um elemento no RDD de sa√≠da.

### Opera√ß√µes
Recebem um ou dois RDDs como entrada e retornam um RDD de sa√≠da
Exemplos:
- **Sample**: retorna um subgrupo do RDD de entrada. √ötil para testes.
- **Distinct**: retorna elementos distintos no RDD de entrada. Essa opera√ß√£o √© bem custosa para ser executada.
- **Union**: retorna a uni√£o dos elementos de dois RDDs. Se tiver elemntos repetidos, eles ser√£o duplicados.
- **Intersection**: retorna elementos comuns de dois RDDs. Lembre-se que se tiver elemntos repetidos dentro de um RDD, eles ser√£o deduplicados.
- **Subtract**: retorna os elementos que existem no primeiro RDD e n√£o existe no segundo.
- **Cartesian product** retorna todos os pares poss√≠veis de elementos entre os dois RDDs.

### A√ß√µes
Computam o resultado baseado no RDD.
Exemplos:
- **First**: retorna o primeiro elemento de um RDD.
- **Collect**: retorna a cole√ß√£o inteira ou o valor para o programa executando os RDDs. Aten√ß√£o pois todos o *Dataset* deve caber na mem√≥ria de apenas uma m√°quina.
- **Count e CountByValue**: *count* vai retornar a quantidade de elementos no *Dataset* e o *CountByValue* vai contar a quantidade de vezes que um elemento aparece no *Dataset*.
- **take**: retorna N elementos de um *Dataset*.
- **reduce**: passa todos os elementos de um *Dataset*, de dois em dois, numa fun√ß√£o de *reduce*. Exemplo se seu *Dataset* √© [1,2,3,4,5] e a fun√ß√£o de reduce for "X*Y" ent√£o o resultado ser√° 120. 
- **SaveAsTextFile**: usado para escrever dados em um sistema de armazenamento distribu√≠do como Amazon S3, HDFS ou at√© no seu sistema de arquivo local.

### B√¥nus: Persist√™ncia
Algumas vezes vamos precisar chamar a√ß√µes no mesmo RDD m√∫ltiplas vezes. Se voc√™ fizer isso de forma ing√™nua, os RDDs e todas suas depend√™ncias ser√£o recomputados cada vez que uma a√ß√£o for chamada num RDD. Se voc√™ for reutilizar um RDD, voc√™ deve persistir chamando o m√©todo **persist()** no seu RDD. Isso vai manter os dados na mem√≥ria e todos os n√≥s do cluster. Existem diferentes n√≠veis para voc√™ persistir os dados levando em considera√ß√£o se ser√° utilizada a mem√≥ria e/ou o disco e se os dados ser√£o serializados ou n√£o.


## Conclus√£o
At√© aqui voc√™ aprendeu conceitos b√°sicos do Spark para trabalhar com um grande volume de dados. Mas ele consegue ser ainda mais poderoso, sendo poss√≠vel adicionar libs de *Machine Learning* ([MLlib](https://spark.apache.org/docs/latest/ml-guide.html)), libs para trabalhar com gr√°fos ([GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html)), escrever sus consultas e agrega√ß√µes no formato SQL ([SparkSQL](https://spark.apache.org/docs/latest/sql-programming-guide.html)) e ainda trabalhar com streaming de dados!

Sobre esse √∫ltimo ponto, existe o [Spark Streaming tamb√©m conhecido como DStreams](https://spark.apache.org/docs/latest/streaming-programming-guide.html) e o [Structured Streaming Programming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html). Eu fiquei bem confuso para entender a diferen√ßa dos dois olhando apenas na documenta√ß√£o oficial, mas esse [link aqui](https://blog.knoldus.com/spark-streaming-vs-structured-streaming/) explica bem a diferen√ßa deles. Em resumo, o Structured Streaming Programming √© mais recomendado para voc√™ que esta utilizando streamings de verdade, o DStreams √© mais para rodar jobs peri√≥dicos.

Gostou de Spark?! Quer colocar em pr√°tica?! A [documenta√ß√£o oficial](https://spark.apache.org/docs/latest/quick-start.html) √© bem completa e f√°cil de seguir. Basta escolher a sua linguag√©m de programa√ß√£o favorita, seguir o tutorial e ser feliz üôÇ.
